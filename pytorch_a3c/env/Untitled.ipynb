{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai2thor_env import AI2ThorDumpEnv\n",
    "import json\n",
    "config = json.load(open(\"../config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objects =  [\"HousePlant\", \"StoveKnob\", \"Sink\", \"TableTop\", \"Potato\", \"Bread\", \"Tomato\", \"Knife\", \"Cabinet\", \"Fridge\", \"Container\", \"ButterKnife\", \"Lettuce\", \"Pan\", \"Bowl\", \"CoffeeMachine\", \"StoveBurner\", \"Plate\"]\n",
    "test_objects =  [\"Mug\", \"Apple\", \"Microwave\", \"Toaster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = AI2ThorDumpEnv(config=config, scenes=[\"FloorPlan1\"], objects=test_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[120, 121, 152, 153]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done resetting.\n"
     ]
    }
   ],
   "source": [
    "state, target = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.30599877, 0.49621511, 0.5104872 , ..., 0.30798587, 0.40391627,\n",
       "        0.31924683]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, done = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.30599877, 0.49621511, 0.5104872 , ..., 0.30798587, 0.40391627,\n",
       "        0.31924683],\n",
       "       [0.30222818, 0.46462259, 0.49328274, ..., 0.32498479, 0.41234699,\n",
       "        0.35560429]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def normalized_columns_initializer(weights, std=1.0):\n",
    "    \"\"\"\n",
    "    Weights are normalized over their column. Also, allows control over std which is useful for\n",
    "    initialising action logit output so that all actions have similar likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    out = torch.randn(weights.size())\n",
    "    out *= std / torch.sqrt(out.pow(2).sum(1, keepdim=True))\n",
    "    return out\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        weight_shape = list(m.weight.data.size())\n",
    "        fan_in = weight_shape[1]\n",
    "        fan_out = weight_shape[0]\n",
    "        w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "        m.weight.data.uniform_(-w_bound, w_bound)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "class ActorCritic(torch.nn.Module):    \n",
    "\n",
    "    def __init__(self, config, num_actions, train_resnet=False, use_gpu=False):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.dtype = torch.FloatTensor if not use_gpu else torch.cuda.FloatTensor\n",
    "        self.train_resnet = train_resnet\n",
    "\n",
    "        if self.train_resnet:\n",
    "            self.extractor = models.resnet50(pretrained=True)\n",
    "            modules = list(self.extractor.children())[:-1]\n",
    "            self.extractor = nn.Sequential(*modules)\n",
    "\n",
    "        self.visual_ft = nn.Linear(in_features=2048 * self.config['history_size'], out_features=512)\n",
    "\n",
    "        if config[\"embeddings\"] is None: \n",
    "            self.semantic_size = 100\n",
    "            self.embeddings = lambda x: np.random.sample((100, )).astype(np.float32)\n",
    "        else:\n",
    "            self.embeddings = pickle.load(open(config[\"embeddings\"], 'rb'))\n",
    "            self.semantic_size = list(self.embeddings.values())[0].shape[0]\n",
    "            \n",
    "        self.semantic_ft = nn.Linear(in_features=self.semantic_size, out_features=512)\n",
    "\n",
    "        if self.config['graph']:\n",
    "            fused_size = 512 * 3\n",
    "        else:\n",
    "            fused_size = 512 * 2\n",
    "\n",
    "        self.hidden_mlp = nn.Linear(in_features=fused_size, out_features=512)\n",
    "        self.critic_linear = nn.Linear(512, 1)\n",
    "        self.actor_linear = nn.Linear(512, num_actions)\n",
    "\n",
    "        self.apply(weights_init)\n",
    "        self.actor_linear.weight.data = normalized_columns_initializer(\n",
    "                                            self.actor_linear.weight.data, 0.01)\n",
    "        self.actor_linear.bias.data.fill_(0)\n",
    "        self.critic_linear.weight.data = normalized_columns_initializer(\n",
    "                                            self.critic_linear.weight.data, 1.0)\n",
    "        self.critic_linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, inputs, word):\n",
    "        assert len(inputs) == self.config['history_size']\n",
    "\n",
    "        inputs = [torch.from_numpy(inp).type(self.dtype) for inp in inputs]\n",
    "        if self.train_resnet:\n",
    "            features = [self.extractor(inp.unsqueeze(0)) for inp in inputs]\n",
    "        else:\n",
    "            features = inputs\n",
    "            \n",
    "        joint_features = torch.cat(features)\n",
    "        joint_features = joint_features.view(1, joint_features.size(0))\n",
    "        visual = F.relu(self.visual_ft(joint_features))\n",
    "        \n",
    "        embeded = torch.from_numpy(self.embeddings[word]).type(self.dtype)\n",
    "        embeded = embeded.view(1, embeded.size(0))\n",
    "        semantic = F.relu(self.semantic_ft(embeded))\n",
    "        joint_embeddings = torch.cat((visual, semantic), 1)\n",
    "        \n",
    "        x = self.hidden_mlp(joint_embeddings)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        return self.critic_linear(x), self.actor_linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCritic(\n",
       "  (visual_ft): Linear(in_features=8192, out_features=512, bias=True)\n",
       "  (semantic_ft): Linear(in_features=300, out_features=512, bias=True)\n",
       "  (hidden_mlp): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (critic_linear): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (actor_linear): Linear(in_features=512, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ActorCritic(config, 6, use_gpu=True)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "value, logit = model(state, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0285, device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [torch.from_numpy(inp).type(torch.cuda.FloatTensor) for inp in state]\n",
    "inputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
